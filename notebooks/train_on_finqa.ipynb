{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pauliusztin/anaconda3/envs/hands-on-llms/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No logging configuration file found at: logging.yaml. Setting logging level to INFO.\n",
      "INFO:training:Initializing...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import comet_ml\n",
    "import sys\n",
    "sys.path.append(\"../modules\")\n",
    "\n",
    "from pathlib import Path\n",
    "from training.api import ChatAPI, FinQATrainingAPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training.api:Loading datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:training.api:Debug mode enabled. Truncating datasets...     \n",
      "INFO:training.api:Loading model...\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.37s/it]\n"
     ]
    }
   ],
   "source": [
    "training_api = FinQATrainingAPI(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pauliusztin/anaconda3/envs/hands-on-llms/lib/python3.9/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/iusztinpaul/hands-on-llms/8c7ad718ee7c4ec3976bb1aaf714ab41\n",
      "\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 07:09, Epoch 9/9]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.327200</td>\n",
       "      <td>2.153886</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.114500</td>\n",
       "      <td>2.132410</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.894500</td>\n",
       "      <td>2.099463</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.155500</td>\n",
       "      <td>2.060199</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.858600</td>\n",
       "      <td>2.021342</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.701300</td>\n",
       "      <td>1.986813</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.842600</td>\n",
       "      <td>1.954988</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.701700</td>\n",
       "      <td>1.925228</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.703500</td>\n",
       "      <td>1.897039</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.760600</td>\n",
       "      <td>1.868288</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>1.607100</td>\n",
       "      <td>1.844457</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.464400</td>\n",
       "      <td>1.829153</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "INFO:accelerate.accelerator:The used dataset had no length, returning gathered tensors. You should drop the remainder yourself.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/iusztinpaul/hands-on-llms/8c7ad718ee7c4ec3976bb1aaf714ab41\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epoch [49]                   : (0.25, 9.0)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_loss [12]               : (1.8291525840759277, 2.153886079788208)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_perplexity [12]         : (5.4978173125164176e-08, 9.951008195230315e-08)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_runtime [12]            : (6.8396, 6.8942)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_samples_per_second [12] : (0.87, 0.877)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     eval_steps_per_second [12]   : (0.87, 0.877)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate                : 0.0002\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss [46]                    : (0.4833844304084778, 2.3272)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     total_flos                   : 2211054892351488.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_loss                   : 1.876427365673913\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_runtime                : 411.8041\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_samples_per_second     : 0.262\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_steps_per_second       : 0.087\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_n_gpu                                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_no_sync_in_gradient_accumulation          : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/_setup_devices                             : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adafactor                                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta1                                 : 0.9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_beta2                                 : 0.999\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/adam_epsilon                               : 1e-08\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/auto_find_batch_size                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16                                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/bf16_full_eval                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/data_seed                                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_drop_last                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_num_workers                     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/dataloader_pin_memory                      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_backend                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_broadcast_buffers                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_bucket_cap_mb                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_find_unused_parameters                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout                                : 1800\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ddp_timeout_delta                          : 0:30:00\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/debug                                      : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed                                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/deepspeed_plugin                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/default_optim                              : adamw_hf\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/device                                     : cuda:0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/disable_tqdm                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/distributed_state                          : Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_eval                                    : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_predict                                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/do_train                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_accumulation_steps                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_batch_size                            : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_delay                                 : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/eval_steps                                 : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/evaluation_strategy                        : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16                                       : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_backend                               : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_full_eval                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fp16_opt_level                             : O1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/framework                                  : pt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp                                       : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_config                                : {'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_min_num_params                        : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/fsdp_transformer_layer_cls_to_wrap         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/full_determinism                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_accumulation_steps                : 3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/gradient_checkpointing                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/greater_is_better                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/group_by_length                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/half_precision_backend                     : auto\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_model_id                               : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_private_repo                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_strategy                               : HubStrategy.EVERY_SAVE\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/hub_token                                  : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ignore_data_skip                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/include_inputs_for_metrics                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/jit_mode_eval                              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_names                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/label_smoothing_factor                     : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/learning_rate                              : 0.0002\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/length_column_name                         : length\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/load_best_model_at_end                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_process_index                        : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/local_rank                                 : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level                                  : passive\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_level_replica                          : warning\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/log_on_each_node                           : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_dir                                : ../logs\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_first_step                         : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_nan_inf_filter                     : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_steps                              : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/logging_strategy                           : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/lr_scheduler_type                          : SchedulerType.CONSTANT\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_grad_norm                              : 0.3\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/max_steps                                  : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/metric_for_best_model                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/mp_parameters                              : \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/n_gpu                                      : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/no_cuda                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/num_train_epochs                           : 9\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim                                      : OptimizerNames.PAGED_ADAMW\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/optim_args                                 : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/output_dir                                 : ../results\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/overwrite_output_dir                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/parallel_mode                              : ParallelMode.NOT_PARALLEL\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/past_index                                 : -1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_eval_batch_size                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_device_train_batch_size                : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_eval_batch_size                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/per_gpu_train_batch_size                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/place_model_on_device                      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/prediction_loss_only                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/process_index                              : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_model_id                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_organization                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/push_to_hub_token                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/ray_scope                                  : last\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/remove_unused_columns                      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/report_to                                  : ['comet_ml']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/resume_from_checkpoint                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/run_name                                   : ../results\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_on_each_node                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_safetensors                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_steps                                 : 40\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_strategy                              : IntervalStrategy.STEPS\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/save_total_limit                           : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/seed                                       : 42\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/sharded_ddp                                : []\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_log                                 : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/should_save                                : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/skip_memory_metrics                        : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tf32                                       : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile                              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_backend                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torch_compile_mode                         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/torchdynamo                                : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_metrics_debug                          : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/tpu_num_cores                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/train_batch_size                           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_cpu                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_ipex                                   : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_legacy_prediction_loop                 : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/use_mps_device                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_ratio                               : 0.03\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/warmup_steps                               : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/weight_decay                               : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     args/world_size                                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_auto_class                              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_commit_hash                             : eb410fb6ffa9028e97adb801f0d6ec46d02f8b07\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/_name_or_path                            : tiiuae/falcon-7b-instruct\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/add_cross_attention                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/alibi                                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/apply_residual_connection_post_layernorm : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/architectures                            : ['RWForCausalLM']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attention_dropout                        : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/attribute_map                            : {'num_hidden_layers': 'n_layer', 'num_attention_heads': 'n_head'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/auto_map                                 : {'AutoConfig': 'tiiuae/falcon-7b-instruct--configuration_RW.RWConfig', 'AutoModelForCausalLM': 'tiiuae/falcon-7b-instruct--modelling_RW.RWForCausalLM'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bad_words_ids                            : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/begin_suppress_tokens                    : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bias                                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/bos_token_id                             : 11\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/chunk_size_feed_forward                  : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/cross_attention_hidden_size              : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/decoder_start_token_id                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/diversity_penalty                        : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/do_sample                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/early_stopping                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/encoder_no_repeat_ngram_size             : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/eos_token_id                             : 11\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/exponential_decay_length_penalty         : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/finetuning_task                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_bos_token_id                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/forced_eos_token_id                      : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/head_dim                                 : 64\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_dropout                           : 0.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/hidden_size                              : 4544\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/id2label                                 : {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/initializer_range                        : 0.02\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_composition                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_decoder                               : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/is_encoder_decoder                       : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/keys_to_ignore_at_inference              : ['past_key_values']\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/label2id                                 : {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/layer_norm_epsilon                       : 1e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/length_penalty                           : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/max_length                               : 20\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/min_length                               : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/model_type                               : RefinedWebModel\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/multi_query                              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/n_head                                   : 71\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/n_layer                                  : 32\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/name_or_path                             : tiiuae/falcon-7b-instruct\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/no_repeat_ngram_size                     : 0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beam_groups                          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_beams                                : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_labels                               : 2\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/num_return_sequences                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_attentions                        : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_hidden_states                     : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/output_scores                            : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pad_token_id                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/parallel_attn                            : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/prefix                                   : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/problem_type                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/pruned_heads                             : {}\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/quantization_config                      : BitsAndBytesConfig {\n",
      "  \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "  \"bnb_4bit_quant_type\": \"nf4\",\n",
      "  \"bnb_4bit_use_double_quant\": true,\n",
      "  \"load_in_4bit\": true\n",
      "}\n",
      "\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/remove_invalid_values                    : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/repetition_penalty                       : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict                              : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/return_dict_in_generate                  : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/rotary                                   : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/sep_token_id                             : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/suppress_tokens                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/task_specific_params                     : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/temperature                              : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tf_legacy_loss                           : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_encoder_decoder                      : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tie_word_embeddings                      : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/tokenizer_class                          : None\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_k                                    : 50\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/top_p                                    : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torch_dtype                              : torch.bfloat16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/torchscript                              : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/transformers_version                     : 4.27.4\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/typical_p                                : 1.0\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_bfloat16                             : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_cache                                : False\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/use_return_dict                          : True\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     config/vocab_size                               : 65024\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     asset                        : 88 (1.71 GB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-environment-definition : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-info                   : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     conda-specification          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details          : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git metadata                 : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     git-patch (uncompressed)     : 1 (23.46 KB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model graph                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook                     : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code                  : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 1 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 17 file(s), remaining 1.27 GB/1.71 GB\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 10 asset(s), remaining 334.42 MB/1.71 GB, Throughput 64.44 MB/s, ETA ~6s\n"
     ]
    }
   ],
   "source": [
    "trainer = training_api.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'loss': 1.0479, 'learning_rate': 0.0002, 'epoch': 1.0, 'step': 1},\n",
       " {'loss': 1.0542, 'learning_rate': 0.0002, 'epoch': 2.0, 'step': 2},\n",
       " {'loss': 1.0474, 'learning_rate': 0.0002, 'epoch': 3.0, 'step': 3},\n",
       " {'eval_loss': 2.1510212421417236,\n",
       "  'eval_perplexity': 6.727502466219448e-08,\n",
       "  'eval_runtime': 5.4704,\n",
       "  'eval_samples_per_second': 1.097,\n",
       "  'eval_steps_per_second': 0.366,\n",
       "  'epoch': 3.0,\n",
       "  'step': 3},\n",
       " {'loss': 1.0546, 'learning_rate': 0.0002, 'epoch': 4.0, 'step': 4},\n",
       " {'loss': 1.0475, 'learning_rate': 0.0002, 'epoch': 5.0, 'step': 5},\n",
       " {'loss': 1.0368, 'learning_rate': 0.0002, 'epoch': 6.0, 'step': 6},\n",
       " {'eval_loss': 2.1235926151275635,\n",
       "  'eval_perplexity': 6.37254586877134e-08,\n",
       "  'eval_runtime': 5.4891,\n",
       "  'eval_samples_per_second': 1.093,\n",
       "  'eval_steps_per_second': 0.364,\n",
       "  'epoch': 6.0,\n",
       "  'step': 6},\n",
       " {'train_runtime': 135.3107,\n",
       "  'train_samples_per_second': 0.399,\n",
       "  'train_steps_per_second': 0.044,\n",
       "  'total_flos': 982691063267328.0,\n",
       "  'train_loss': 1.0480629007021587,\n",
       "  'epoch': 6.0,\n",
       "  'step': 6}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = finqa.FinQADataset(\n",
    "#     data_path=ROOT_DATASET_DIR / \"private_test.json\", scope=constants.Scope.TESTING\n",
    "# ).to_huggingface()\n",
    "# test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_api = ChatAPI(peft_model_id=Path(\"..\") / \"results\" / \"checkpoint-280\", max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\n",
    "#     chat_api.ask(\n",
    "#         question=test_dataset[\"text\"][0],\n",
    "#     )\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hands-on-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
